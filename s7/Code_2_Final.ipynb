{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6aLlCIxZMp94"
      },
      "source": [
        "# Code 2\n",
        "\n",
        "Please find the notes at the end\n",
        "\n",
        "![](https://1.bp.blogspot.com/-HGCac9oDqdI/XOxeOCB0E0I/AAAAAAAAQl4/zkGtCTlFUbIvg3PA_q2csMxsUgH1sQBuQCLcBGAs/s1600/IMG_2294.JPG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofNRIzCGRyi3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62P9WuXYSyxH"
      },
      "outputs": [],
      "source": [
        "# Train data transformation\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.RandomRotation((-6.9, 6.9), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "                                    ])\n",
        "\n",
        "# Test data transformations\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MZi5PZFVmPJ"
      },
      "outputs": [],
      "source": [
        "# download dataset and create train and test sets separately\n",
        "\n",
        "train = datasets.MNIST(root = '../data', train = True, download = True, transform= train_transforms)\n",
        "\n",
        "test = datasets.MNIST(root = '../data', train = False, download = True, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB7zRmGXZJTe"
      },
      "outputs": [],
      "source": [
        "# set seed values\n",
        "torch.manual_seed(400)\n",
        "dropout = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvuLYjgpbtnX"
      },
      "outputs": [],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(400)\n",
        "    \n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrmJzVIyb1UE"
      },
      "outputs": [],
      "source": [
        "dataloader_args = dict(shuffle=True, batch_size = 64, num_workers=2, pin_memory=True) if cuda else dict(shuffle=True, batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1ksUfJ7cE1W"
      },
      "outputs": [],
      "source": [
        "#Dataloaders\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train, **dataloader_args)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test, **dataloader_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bJynJC6c_er"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# plt.imshow(images[1].numpy().squeeze(), cmap='gray_r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6VLrl9_daks"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracy = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_07A_WGdpF_"
      },
      "outputs": [],
      "source": [
        "def train_eval_model(model, train_loader, optimizer, device, epochs=1, test=False, test_loader=None):\n",
        "          \n",
        "    model.train() # set the train mode\n",
        "    \n",
        "    # iterate over for `epochs` epochs and keep storing valuable info\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      correct = processed = train_loss = 0\n",
        "\n",
        "      print(f\"\\n epoch num ================================= {epoch+1}\")\n",
        "      \n",
        "      pbar = tqdm(train_loader)\n",
        "      \n",
        "      for batch_idx, (data, target) in enumerate(pbar):\n",
        "          data, target = data.to(device), target.to(device) # move data to `device`\n",
        "\n",
        "          optimizer.zero_grad() # zero out the gradients to avoid accumulating them over loops\n",
        "\n",
        "          output = model(data) # get the model's predictions\n",
        "\n",
        "          loss = f.nll_loss(output, target) # calculate Negative Log Likelihood loss using ground truth labels and the model's predictions\n",
        "\n",
        "          train_loss += loss.item() # add up the train loss\n",
        "\n",
        "          loss.backward() # boom ! The magic function to perform backpropagation and calculate the gradients\n",
        "\n",
        "          optimizer.step() # take 1 step foe the optimizer and update the weights\n",
        "\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item() #  compare and see how many predictions are coorect and then add up the count\n",
        "\n",
        "          processed += len(data) # total processed data size\n",
        "\n",
        "      acc = 100 * correct/processed\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "      train_accuracy.append(acc)\n",
        "      \n",
        "      pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "      \n",
        "      \n",
        "      train_loss /= len(train_loader.dataset)\n",
        "      print('Train metrics: accuracy: {}/{} ({:.4f}%)'.format(correct,\n",
        "                                                              len(train_loader.dataset),\n",
        "                                                              correct * 100 / len(train_loader.dataset)))\n",
        "\n",
        "      if test: # moving to evaluation\n",
        "          model.eval() # set the correct mode\n",
        "          \n",
        "          correct = test_loss = 0\n",
        "\n",
        "          with torch.no_grad(): # to disable gradient calculation with no_grad context\n",
        "              \n",
        "              for data, target in test_loader:\n",
        "\n",
        "                  data, target = data.to(device), target.to(device)\n",
        "                  \n",
        "                  output = model(data)\n",
        "                  \n",
        "                  test_loss += f.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "                  \n",
        "                  pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "                  \n",
        "                  correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "          test_loss /= len(test_loader.dataset)\n",
        "          test_losses.append(test_loss)\n",
        "          test_accuracy.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "          print('Test metrics: average loss: {:.4f}, accuracy: {}/{} ({:.5f}%)\\n'.format(\n",
        "              test_loss, correct, len(test_loader.dataset),\n",
        "              100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZTnhFeCip1D"
      },
      "outputs": [],
      "source": [
        "class Network_2(nn.Module):        \n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=12, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)) #input = 28, output = 28, RF = 3\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(12, 16, 3, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)) #input = 28, output = 26, RF = 5\n",
        "        \n",
        "        # TRANSITION BLOCK 1, let's have a mix of channels without extracting features here\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0, bias=False),\n",
        "            nn.ReLU()) #input = 28, output = 26, RF = 5\n",
        "        \n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input = 26, output = 13, RF = 10\n",
        "\n",
        "        \n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(10, 16, 3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout)) #input = 13, output = 11, RF = 12\n",
        "\n",
        "        \n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout)) #input = 11, output = 9, RF = 14\n",
        "        \n",
        "        # Transition via 1x1 to reduce params and allow selection of relevant channels for next 3x3 layer to extract features\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(dropout)) #input = 9, output = 9, RF = 14\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(10, 16, 3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout)) #input = 9, output = 7, RF = 16\n",
        "        \n",
        "        self.avg = nn.AvgPool2d(7) # Average Pool layer to reduce dimensions and have a larger view for incoming dimensions to make a decision\n",
        "        \n",
        "        # Final layer with 1x1 to have 10 output channels\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0, bias=False)) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x) \n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.avg(x)\n",
        "        x = self.convblock8(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return f.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO7ne6w1kSx2",
        "outputId": "7a472a3d-d601-4be7-b689-761f3dcee179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 12, 28, 28]             108\n",
            "       BatchNorm2d-2           [-1, 12, 28, 28]              24\n",
            "              ReLU-3           [-1, 12, 28, 28]               0\n",
            "           Dropout-4           [-1, 12, 28, 28]               0\n",
            "            Conv2d-5           [-1, 16, 26, 26]           1,728\n",
            "       BatchNorm2d-6           [-1, 16, 26, 26]              32\n",
            "              ReLU-7           [-1, 16, 26, 26]               0\n",
            "           Dropout-8           [-1, 16, 26, 26]               0\n",
            "            Conv2d-9           [-1, 10, 26, 26]             160\n",
            "             ReLU-10           [-1, 10, 26, 26]               0\n",
            "        MaxPool2d-11           [-1, 10, 13, 13]               0\n",
            "           Conv2d-12           [-1, 16, 11, 11]           1,440\n",
            "             ReLU-13           [-1, 16, 11, 11]               0\n",
            "      BatchNorm2d-14           [-1, 16, 11, 11]              32\n",
            "          Dropout-15           [-1, 16, 11, 11]               0\n",
            "           Conv2d-16             [-1, 16, 9, 9]           2,304\n",
            "             ReLU-17             [-1, 16, 9, 9]               0\n",
            "      BatchNorm2d-18             [-1, 16, 9, 9]              32\n",
            "          Dropout-19             [-1, 16, 9, 9]               0\n",
            "           Conv2d-20             [-1, 10, 9, 9]             160\n",
            "             ReLU-21             [-1, 10, 9, 9]               0\n",
            "      BatchNorm2d-22             [-1, 10, 9, 9]              20\n",
            "          Dropout-23             [-1, 10, 9, 9]               0\n",
            "           Conv2d-24             [-1, 16, 7, 7]           1,440\n",
            "             ReLU-25             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-26             [-1, 16, 7, 7]              32\n",
            "          Dropout-27             [-1, 16, 7, 7]               0\n",
            "        AvgPool2d-28             [-1, 16, 1, 1]               0\n",
            "           Conv2d-29             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 7,672\n",
            "Trainable params: 7,672\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.88\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.91\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# check model summary\n",
        "model = Network_2().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kY280erkXzu",
        "outputId": "432edd2f-2ae7-4e5c-b7d4-7d17f3b51d68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch num ================================= 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:25<00:00, 37.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 52391/60000 (87.3183%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.1021, accuracy: 9687/10000 (96.87000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:24<00:00, 38.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 58002/60000 (96.6700%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0570, accuracy: 9818/10000 (98.18000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 58570/60000 (97.6167%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0399, accuracy: 9877/10000 (98.77000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:24<00:00, 39.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 58874/60000 (98.1233%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0489, accuracy: 9848/10000 (98.48000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 58991/60000 (98.3183%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0340, accuracy: 9889/10000 (98.89000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59101/60000 (98.5017%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0366, accuracy: 9883/10000 (98.83000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59210/60000 (98.6833%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0355, accuracy: 9893/10000 (98.93000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59253/60000 (98.7550%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0485, accuracy: 9863/10000 (98.63000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59263/60000 (98.7717%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0288, accuracy: 9910/10000 (99.10000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59316/60000 (98.8600%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0308, accuracy: 9907/10000 (99.07000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59345/60000 (98.9083%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0287, accuracy: 9915/10000 (99.15000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59350/60000 (98.9167%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0278, accuracy: 9904/10000 (99.04000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 39.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59400/60000 (99.0000%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0258, accuracy: 9917/10000 (99.17000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:23<00:00, 40.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train metrics: accuracy: 59392/60000 (98.9867%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test metrics: average loss: 0.0289, accuracy: 9912/10000 (99.12000%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
        "\n",
        "train_eval_model(model, train_loader, optimizer, device, epochs=14, test=True, test_loader=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMCbytKpSK-b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgekG-0N4aGI"
      },
      "source": [
        "# Code 2 Notes:\n",
        "\n",
        "## Target:\n",
        "To have reduced gap between train and test accuracies as well as push model towards 99.4% target, with help of Dropout and Image Augmentation to recude over-fitting as well as ease out the variances with augmentation and help network learn better.\n",
        "\n",
        "## Result:\n",
        "Test accuracy crosses 99% and has consistency in last epochs with train accuracy dropping down and thus lowering the gap between the two.\n",
        "Final train accuracy: 98.98%\n",
        "Final test accuracy : 99.12% \n",
        "> when trained and tested for 14 epochs\n",
        "\n",
        "\n",
        "## Analysis:\n",
        "The results are still not great, need to further use advanced approaches to further increase test accuracy.\n",
        "\n",
        "Important point to note is that the gap between the two accuracies has reduced a lot in comparison to previous code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4a-PbCS3Q5o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EVA 6 S5 Code 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
