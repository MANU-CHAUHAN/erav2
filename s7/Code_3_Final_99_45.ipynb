{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6aLlCIxZMp94"
      },
      "source": [
        "# Code 3\n",
        "\n",
        "Please find the notes at the end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ofNRIzCGRyi3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "62P9WuXYSyxH"
      },
      "outputs": [],
      "source": [
        "# Train data transformation\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.RandomRotation((-6.9, 6.9), fill=(1,)),\n",
        "                                       transforms.RandomAffine(degrees=15), #translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "                                    ])\n",
        "\n",
        "# Test data transformations\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1MZi5PZFVmPJ"
      },
      "outputs": [],
      "source": [
        "# download dataset and create train and test sets separately\n",
        "\n",
        "train = datasets.MNIST(root = '../data', train = True, download = True, transform= train_transforms)\n",
        "\n",
        "test = datasets.MNIST(root = '../data', train = False, download = True, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oB7zRmGXZJTe"
      },
      "outputs": [],
      "source": [
        "# set seed values\n",
        "torch.manual_seed(400)\n",
        "dropout = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SvuLYjgpbtnX"
      },
      "outputs": [],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(400)\n",
        "    \n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qrmJzVIyb1UE"
      },
      "outputs": [],
      "source": [
        "dataloader_args = dict(shuffle=True, batch_size = 64, num_workers=2, pin_memory=True) if cuda else dict(shuffle=True, batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x1ksUfJ7cE1W"
      },
      "outputs": [],
      "source": [
        "#Dataloaders\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train, **dataloader_args)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test, **dataloader_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9bJynJC6c_er"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# plt.imshow(images[1].numpy().squeeze(), cmap='gray_r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J6VLrl9_daks"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracy = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7_07A_WGdpF_"
      },
      "outputs": [],
      "source": [
        "def train_eval_model(model, train_loader, optimizer, device, epochs=1, test=False, test_loader=None, scheduler=None):\n",
        "          \n",
        "    model.train() # set the train mode\n",
        "    \n",
        "    # iterate over for `epochs` epochs and keep storing valuable info\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      correct = processed = train_loss = 0\n",
        "    #   if scheduler:\n",
        "    #     scheduler.step()\n",
        "\n",
        "      print(f\"\\n epoch num ================================= {epoch+1}\")\n",
        "      \n",
        "      pbar = tqdm(train_loader)\n",
        "      \n",
        "      for batch_idx, (data, target) in enumerate(pbar):\n",
        "          data, target = data.to(device), target.to(device) # move data to `device`\n",
        "\n",
        "          optimizer.zero_grad() # zero out the gradients to avoid accumulating them over loops\n",
        "\n",
        "          output = model(data) # get the model's predictions\n",
        "\n",
        "          loss = f.nll_loss(output, target) # calculate Negative Log Likelihood loss using ground truth labels and the model's predictions\n",
        "\n",
        "          train_loss += loss.item() # add up the train loss\n",
        "\n",
        "          loss.backward() # boom ! The magic function to perform backpropagation and calculate the gradients\n",
        "\n",
        "          optimizer.step() # take 1 step for the optimizer and update the weights\n",
        "\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item() #  compare and see how many predictions are coorect and then add up the count\n",
        "\n",
        "          processed += len(data) # total processed data size\n",
        "\n",
        "      acc = 100 * correct/processed\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "\n",
        "      train_accuracy.append(acc)\n",
        "\n",
        "      if scheduler:\n",
        "        print(\"\\n\\n\\t\\t\\tLast LR -->\", scheduler.get_last_lr())\n",
        "        scheduler.step()\n",
        "\n",
        "      pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "      \n",
        "      \n",
        "      train_loss /= len(train_loader.dataset)\n",
        "      print('\\n\\t\\t\\tTrain metrics: accuracy: {}/{} ({:.4f}%)'.format(correct,\n",
        "                                                              len(train_loader.dataset),\n",
        "                                                              correct * 100 / len(train_loader.dataset)))\n",
        "\n",
        "      if test: # moving to evaluation\n",
        "          model.eval() # set the correct mode\n",
        "          \n",
        "          correct = test_loss = 0\n",
        "\n",
        "          with torch.no_grad(): # to disable gradient calculation with no_grad context\n",
        "              \n",
        "              for data, target in test_loader:\n",
        "\n",
        "                  data, target = data.to(device), target.to(device)\n",
        "                  \n",
        "                  output = model(data)\n",
        "                  \n",
        "                  test_loss += f.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "                  \n",
        "                  pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "                  \n",
        "                  correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "          test_loss /= len(test_loader.dataset)\n",
        "          test_losses.append(test_loss)\n",
        "          test_accuracy.append(100. * correct / len(test_loader.dataset))\n",
        "\n",
        "          print('\\n\\tTest metrics: average loss: {:.4f}, accuracy: {}/{} ({:.5f}%)\\n'.format(\n",
        "              test_loss, correct, len(test_loader.dataset),\n",
        "              100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cZTnhFeCip1D"
      },
      "outputs": [],
      "source": [
        "class Network3(nn.Module):        \n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=12, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)) #input = 28, output = 28, RF = 3\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(12, 16, 3, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)) #input = 28, output = 26, RF = 5\n",
        "        \n",
        "        # TRANSITION BLOCK 1, let's have a mix of channels without extracting features here\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0, bias=False),\n",
        "            nn.ReLU()) #input = 28, output = 26, RF = 5\n",
        "        \n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input = 26, output = 13, RF = 10\n",
        "\n",
        "        \n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(10, 16, 3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout)) #input = 13, output = 11, RF = 12\n",
        "\n",
        "        \n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout)) #input = 11, output = 9, RF = 14\n",
        "        \n",
        "        # Transition via 1x1 to reduce params and allow selection of relevant channels for next 3x3 layer to extract features\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(dropout)) #input = 9, output = 9, RF = 14\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(10, 16, 3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout)) #input = 9, output = 7, RF = 16\n",
        "        \n",
        "        self.avg = nn.AvgPool2d(7) # Average Pool layer to reduce dimensions and have a larger view for incoming dimensions to make a decision\n",
        "        \n",
        "        # Final layer with 1x1 to have 10 output channels\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0, bias=False)) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x) \n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.avg(x)\n",
        "        x = self.convblock8(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return f.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO7ne6w1kSx2",
        "outputId": "db36d3e9-2cf0-4575-cd45-2d714a2d42ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 12, 28, 28]             108\n",
            "       BatchNorm2d-2           [-1, 12, 28, 28]              24\n",
            "              ReLU-3           [-1, 12, 28, 28]               0\n",
            "           Dropout-4           [-1, 12, 28, 28]               0\n",
            "            Conv2d-5           [-1, 16, 26, 26]           1,728\n",
            "       BatchNorm2d-6           [-1, 16, 26, 26]              32\n",
            "              ReLU-7           [-1, 16, 26, 26]               0\n",
            "           Dropout-8           [-1, 16, 26, 26]               0\n",
            "            Conv2d-9           [-1, 10, 26, 26]             160\n",
            "             ReLU-10           [-1, 10, 26, 26]               0\n",
            "        MaxPool2d-11           [-1, 10, 13, 13]               0\n",
            "           Conv2d-12           [-1, 16, 11, 11]           1,440\n",
            "             ReLU-13           [-1, 16, 11, 11]               0\n",
            "      BatchNorm2d-14           [-1, 16, 11, 11]              32\n",
            "          Dropout-15           [-1, 16, 11, 11]               0\n",
            "           Conv2d-16             [-1, 16, 9, 9]           2,304\n",
            "             ReLU-17             [-1, 16, 9, 9]               0\n",
            "      BatchNorm2d-18             [-1, 16, 9, 9]              32\n",
            "          Dropout-19             [-1, 16, 9, 9]               0\n",
            "           Conv2d-20             [-1, 10, 9, 9]             160\n",
            "             ReLU-21             [-1, 10, 9, 9]               0\n",
            "      BatchNorm2d-22             [-1, 10, 9, 9]              20\n",
            "          Dropout-23             [-1, 10, 9, 9]               0\n",
            "           Conv2d-24             [-1, 16, 7, 7]           1,440\n",
            "             ReLU-25             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-26             [-1, 16, 7, 7]              32\n",
            "          Dropout-27             [-1, 16, 7, 7]               0\n",
            "        AvgPool2d-28             [-1, 16, 1, 1]               0\n",
            "           Conv2d-29             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 7,672\n",
            "Trainable params: 7,672\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.88\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.91\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# check model summary\n",
        "model = Network3().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kY280erkXzu",
        "outputId": "298f4a11-3842-4019-a70a-027e0141ca83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch num ================================= 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:27<00:00, 33.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.075]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 55526/60000 (92.5433%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0629, accuracy: 9811/10000 (98.11000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:27<00:00, 33.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.04875]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 58340/60000 (97.2333%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0720, accuracy: 9755/10000 (97.55000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:27<00:00, 33.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.0316875]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 58729/60000 (97.8817%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0371, accuracy: 9896/10000 (98.96000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:27<00:00, 33.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.020596875]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 58984/60000 (98.3067%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0378, accuracy: 9882/10000 (98.82000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.013387968750000001]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59102/60000 (98.5033%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0240, accuracy: 9931/10000 (99.31000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.0087021796875]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59217/60000 (98.6950%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0219, accuracy: 9942/10000 (99.42000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.005656416796875001]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59251/60000 (98.7517%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0211, accuracy: 9935/10000 (99.35000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.003676670917968751]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59266/60000 (98.7767%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0216, accuracy: 9936/10000 (99.36000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.0023898360966796883]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59324/60000 (98.8733%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0211, accuracy: 9937/10000 (99.37000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.0015533934628417976]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59313/60000 (98.8550%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0202, accuracy: 9942/10000 (99.42000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.0010097057508471684]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59337/60000 (98.8950%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0203, accuracy: 9944/10000 (99.44000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.0006563087380506594]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59336/60000 (98.8933%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0197, accuracy: 9945/10000 (99.45000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:27<00:00, 33.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.00042660067973292865]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59334/60000 (98.8900%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0201, accuracy: 9945/10000 (99.45000%)\n",
            "\n",
            "\n",
            " epoch num ================================= 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:28<00:00, 33.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\t\tLast LR --> [0.0002772904418264036]\n",
            "\n",
            "\t\t\tTrain metrics: accuracy: 59354/60000 (98.9233%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tTest metrics: average loss: 0.0198, accuracy: 9945/10000 (99.45000%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.075,momentum=0.9, nesterov=True)\n",
        "scheduler = StepLR(optimizer=optimizer, step_size=1, gamma=0.65)\n",
        "\n",
        "train_eval_model(model, train_loader, optimizer, device, epochs=14, test=True, test_loader=test_loader,scheduler=scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YMCbytKpSK-b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgekG-0N4aGI"
      },
      "source": [
        "# Code 3 Notes:\n",
        "\n",
        "## Target:\n",
        "To have test accuracy consistently be above 99.40% (at least last 4 epochs), push the learning capacity of the model further while ensuring the gap between the two accuracies stays close.\n",
        "\n",
        "## Result:\n",
        "Test accuracy crosses 99% and has consistency in last epochs with train accuracy being below 99% mark, thus having some more learning tendency, perhaps more epochs and experiments with LR could train accuracy more.\n",
        "Final train accuracy: 98.92%\n",
        "Final test accuracy : 99.45% \n",
        ">  trained and tested for 14 epochs\n",
        "\n",
        "\n",
        "## Analysis:\n",
        "Dropout (along with Augmentation) definitely played it's role in ensuring that the gap between two accuracies lowers down, resulting in lower train accuracy and allowing network to be pushed further along with test accuracy. Augmentation and Dropout have not only avoided over-fitting, they have resulted in very small level of under-fitting as model could definitely be pushed further as per train accuracy values.\n",
        "\n",
        "StepLR is another main concept implemented in code 3 and thus allowed to decrease LR with increasing epochs. Had to experiment with multiple combinations of LR and Gamma though.\n",
        "\n",
        "Train accuracy is still not high as expected. \n",
        "\n",
        "Further experiments with augmentation can be done. Dropout value has to be fixed in this case but it's position can be experimented with further.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "n4a-PbCS3Q5o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EVA-6 S5 Code 3 Final- 99_45.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
